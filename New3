from selenium import webdriver
from selenium.webdriver.edge.service import Service
from selenium.webdriver.edge.options import Options
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException, TimeoutException
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
from openpyxl import Workbook

# Dictionary with keys and multiple values
data = {
    "key1": ["G008020919"],
    "key2": ["060062363"],
    "key1": ["1234421"]
}

# Configure Edge options
edge_options = Options()
edge_options.add_experimental_option("debuggerAddress", "127.0.0.1:9222")

# Initialize Edge driver
driver = webdriver.Edge(options=edge_options)

# Initialize results list
results = []

def find_element_with_css(driver, css_selector, timeout=10):
    """Helper function to find elements using CSS selectors with proper waiting"""
    try:
        element = WebDriverWait(driver, timeout).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, css_selector))
        )
        return element
    except TimeoutException:
        return None

def analyze_page_structure(driver, value):
    """Analyze the page structure and determine if the item exists"""
    try:
        # Wait for page load indicator (adjust selector as needed)
        WebDriverWait(driver, 10).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        time.sleep(2)  # Additional safety delay

        # Print page title for debugging
        print(f"\nPage Title: {driver.title}")
        
        # Get all elements that might indicate search results
        search_indicators = {
            'results_count': '//div[contains(@class, "results-count")]',
            'product_grid': '//div[contains(@class, "product-grid")]',
            'no_results': '//div[contains(@class, "no-results")]',
            'search_results': '//div[contains(@class, "search-results")]'
        }

        found_elements = {}
        for name, xpath in search_indicators.items():
            try:
                elements = driver.find_elements(By.XPATH, xpath)
                if elements:
                    found_elements[name] = [elem.text for elem in elements]
            except:
                continue

        print("\nFound page elements:")
        for name, texts in found_elements.items():
            print(f"{name}: {texts}")

        # Look for common product indicators
        product_exists = any([
            driver.find_elements(By.CSS_SELECTOR, '.product-item'),
            driver.find_elements(By.CSS_SELECTOR, '.product-card'),
            driver.find_elements(By.CSS_SELECTOR, '[data-testid*="product"]'),
            driver.find_elements(By.CSS_SELECTOR, '[class*="product"]')
        ])

        # Look for common "no results" indicators
        no_results_exists = any([
            driver.find_elements(By.CSS_SELECTOR, '[class*="no-result"]'),
            driver.find_elements(By.CSS_SELECTOR, '[class*="searchfail"]'),
            driver.find_elements(By.CSS_SELECTOR, '[class*="empty-search"]')
        ])

        if product_exists:
            return "Present in website"
        elif no_results_exists:
            return "Not present in website"
        else:
            # If neither is found, let's get all classes for debugging
            elements = driver.find_elements(By.XPATH, "//*[@class]")
            classes = set()
            for element in elements:
                try:
                    class_list = element.get_attribute("class").split()
                    classes.update(class_list)
                except:
                    continue
            
            print("\nAll classes found on page:")
            print(sorted(classes))
            return "Status unclear - check debug output"

    except Exception as e:
        print(f"Error analyzing page: {str(e)}")
        return "Error analyzing page"

try:
    for key, values in data.items():
        for value in values:
            url = f"https://direct.asda.com/george/clothing/10,default,sc.html?q={value}"
            print(f"\nChecking URL: {url}")
            driver.get(url)
            
            status = analyze_page_structure(driver, value)
            results.append((key, value, status))
            print(f"Result for {value}: {status}")
            
            time.sleep(2)  # Delay between requests

finally:
    # Save results to an Excel file
    wb = Workbook()
    ws = wb.active
    ws.title = "Search Results"
    
    # Write header row
    ws.append(["Key", "Value", "Status"])
    
    # Write data rows
    for key, value, status in results:
        ws.append([key, value, status])
    
    # Save the Excel file
    output_file = "search_results.xlsx"
    wb.save(output_file)
    print(f"\nResults saved to {output_file}")
